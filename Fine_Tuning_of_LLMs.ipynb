{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiu42/Fine-Tuning-of-LLMs-with-Hugging-Face/blob/main/Fine_Tuning_of_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning of LLMs with Hugging Face"
      ],
      "metadata": {
        "id": "WVa0caPZlogN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Installing and importing the libraries for Hugging Face"
      ],
      "metadata": {
        "id": "fT5BjFcflZAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLXwJqbjtPho"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "eRZm_OAbs3qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging)"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Setting up links to Hugging Face datasets and models"
      ],
      "metadata": {
        "id": "CMO9Ew1534fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Setting up all the QLoRA hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "OQA3fMY647SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Setting up all the bitsandbytes hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "YFfD46YB2ffY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Setting up all the training arguments hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "_66BRztm29wJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Setting up all the supervised fine-tuning arguments hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "3cmmRIgc3Ree"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Loading the dataset"
      ],
      "metadata": {
        "id": "QEsqc9gasp10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Defining the QLoRA configuration"
      ],
      "metadata": {
        "id": "z0tKj05usvyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Loading the pre-trained LLaMA 2 model"
      ],
      "metadata": {
        "id": "rz3vMSzhs-P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Loading the pre-trained tokenizer for the LLaMA 2 model"
      ],
      "metadata": {
        "id": "g6aWb1e7tNRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Setting up the configuration for the LoRA fine-tuning method"
      ],
      "metadata": {
        "id": "yxOOwMvStaDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Creating a training configuration by setting the training parameters"
      ],
      "metadata": {
        "id": "coUlIR-ytjiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Creating the Supervised Fine-Tuning Trainer"
      ],
      "metadata": {
        "id": "x-5-Thu0tpfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Training the model"
      ],
      "metadata": {
        "id": "oSF8SHFKt1xL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Chatting with the model"
      ],
      "metadata": {
        "id": "XMPw6WU6vbjP"
      }
    }
  ]
}
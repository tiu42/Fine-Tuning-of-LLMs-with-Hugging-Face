{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiu42/Fine-Tuning-of-LLMs-with-Hugging-Face/blob/main/Fine_Tuning_of_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning of LLMs with Hugging Face"
      ],
      "metadata": {
        "id": "WVa0caPZlogN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Installing and importing the libraries for Hugging Face"
      ],
      "metadata": {
        "id": "fT5BjFcflZAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLXwJqbjtPho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c399ae6-9c3e-4ff5-881a-4b4a6d959ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "eRZm_OAbs3qA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e72106a-a6b2-4ec5-9253-f94787271bc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging)"
      ],
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Setting up links to Hugging Face datasets and models"
      ],
      "metadata": {
        "id": "CMO9Ew1534fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " model_identifier = \"aboonaji/llama2finetune-v2\"\n",
        " source_dataset = \"gamino/wiki_medical_terms\"\n",
        " formated_dataset = \"aboonaji/wiki_medical_terms_llam2_format\""
      ],
      "metadata": {
        "id": "eM-cHQz_5qiS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Setting up all the QLoRA hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "OQA3fMY647SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_hyper_r = 64\n",
        "lora_hyper_alpha = 16\n",
        "lora_hyper_dropout = 0.1"
      ],
      "metadata": {
        "id": "2NGTchlT7j4K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Setting up all the bitsandbytes hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "YFfD46YB2ffY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enable_4bit = True\n",
        "compute_dtype_bnb = \"float16\"\n",
        "quant_type_bnb = \"nf4\"\n",
        "double_quant_flag = False"
      ],
      "metadata": {
        "id": "YDEBI_Oj8OsF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Setting up all the training arguments hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "_66BRztm29wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_dir = \"./results\"\n",
        "epochs_count = 10\n",
        "enable_fp16 = False\n",
        "enable_bf16 = False\n",
        "train_batch_size = 4\n",
        "eval_batch_size = 4\n",
        "accumulation_steps = 1\n",
        "checkpointing_flag = True\n",
        "grad_norm_limit = 0.3\n",
        "train_learning_rate = 2e-4\n",
        "decay_rate = 0.001\n",
        "optimizer_type = \"paged_adamw_32bit\"\n",
        "scheduler_type = \"cosine\"\n",
        "steps_limit = 100\n",
        "warmup_percentage = 0.03\n",
        "length_grouping = True\n",
        "checkpoint_interval = 0\n",
        "log_interval = 25"
      ],
      "metadata": {
        "id": "AVO0p3869soQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Setting up all the supervised fine-tuning arguments hyperparameters for fine-tuning"
      ],
      "metadata": {
        "id": "3cmmRIgc3Ree"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enable_packing = False\n",
        "sequence_length_max = None\n",
        "device_assignment = {\"\":0}"
      ],
      "metadata": {
        "id": "gjDVzRJ3_3sG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Loading the dataset"
      ],
      "metadata": {
        "id": "QEsqc9gasp10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Defining the QLoRA configuration"
      ],
      "metadata": {
        "id": "z0tKj05usvyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Loading the pre-trained LLaMA 2 model"
      ],
      "metadata": {
        "id": "rz3vMSzhs-P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Loading the pre-trained tokenizer for the LLaMA 2 model"
      ],
      "metadata": {
        "id": "g6aWb1e7tNRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Setting up the configuration for the LoRA fine-tuning method"
      ],
      "metadata": {
        "id": "yxOOwMvStaDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Creating a training configuration by setting the training parameters"
      ],
      "metadata": {
        "id": "coUlIR-ytjiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Creating the Supervised Fine-Tuning Trainer"
      ],
      "metadata": {
        "id": "x-5-Thu0tpfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Training the model"
      ],
      "metadata": {
        "id": "oSF8SHFKt1xL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 15: Chatting with the model"
      ],
      "metadata": {
        "id": "XMPw6WU6vbjP"
      }
    }
  ]
}